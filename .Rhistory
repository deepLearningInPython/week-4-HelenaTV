print(tokens)
text = "The quick brown fox jumps over the lazy dog!"
tokens = [''.join(letter.lower() for letter in word if letter.isalpha())
print(tokens)
tokens = [''.join(letter.lower() for letter in word if letter.isalpha()) for word in text.split() if any(letter.isalpha() for letter in word)]
print(tokens)
def tokenize(string: str) -> list:
tokenized_string = []
string.split()
for word in string:
new_word = ""
for letter in word:
if letter.isalpha()
new_word = + letter.lower()
tokenized_string.append(new_word)
return tokenized_string
def tokenize(string: str) -> list:
tokenized_string = []
string.split()
for word in string:
new_word = ""
for letter in word:
if letter.isalpha()
new_word = + letter.lower()
tokenized_string.append(new_word)
return tokenized_string
def tokenize(string: str) -> list:
tokenized_string = []
string.split()
for word in string:
new_word = ""
for letter in word:
if letter.isalpha()
new_word = + letter.lower()
tokenized_string.append(new_word)
return tokenized_string
def tokenize(string: str):
tokenized_string = []
string.split()
for word in string:
new_word = ""
for letter in word:
if letter.isalpha()
new_word = + letter.lower()
tokenized_string.append(new_word)
return tokenized_string
def tokenize(string: str) -> list:
tokenized_string = []
for word in string.split():
new_word = ""
for letter in word:
if letter.isalpha()
new_word = + letter.lower()
tokenized_string.append(new_word)
return tokenized_string
def tokenize(string: str) -> list:
tokenized_string = []
for word in string.split():
new_word = ""
for letter in word:
if letter.isalpha():
new_word = + letter.lower()
tokenized_string.append(new_word)
return tokenized_string
tokenize(text)
def tokenize(string: str) -> list:
tokenized_string = []
for word in string.split():
new_word = ""
for letter in word:
if letter.isalpha():
new_word = + letter.lower()
tokenized_string.append(new_word)
return tokenized_string
def tokenize(string: str) -> list:
tokenized_string = []
for word in string.split():
new_word = ""
for letter in word:
if letter.isalpha():
new_word = + letter.lower()
tokenized_string.append(new_word)
return tokenized_string
def tokenize(string: str) -> list:
tokenized_string = []
for word in string.split():
new_word = ""
for letter in word:
if letter.isalpha():
new_word = + letter.lower()
tokenized_string.append(new_word)
return tokenized_string
tokenize(text)
tokenize(text)
def tokenize(string: str) -> list:
tokenized_string = []
for word in string.split():
new_word = ""
for letter in word:
if letter.isalpha():
new_word +=letter.lower()
tokenized_string.append(new_word)
return tokenized_string
tokenize(text)
def tokenize(string: str) -> list:
tokenized_string = []
for word in string.split():
new_word = ""
for letter in word:
if letter.isalpha():
new_word +=letter.lower()
if new_word:
tokenized_string.append(new_word)
return tokenized_string
tokenize(text)
tokenize(text)
def tokenize(string: str) -> list:
tokenized_string = []
for word in string.split():
new_word = ""
for letter in word:
if letter.isalpha():
new_word +=letter.lower()
if new_word:
tokenized_string.append(new_word)
return tokenized_string.sort()
tokenize(text)
tokenize(text)
def tokenize(string: str) -> list:
tokenized_string = []
for word in string.split():
new_word = ""
for letter in word:
if letter.isalpha():
new_word +=letter.lower()
if new_word:
tokenized_string.append(new_word)
return tokenized_string
tokenize(text)
def tokenize(string: str) -> list:
tokenized_string = []
for word in string.split():
new_word = ""
for letter in word:
if letter.isalpha():
new_word +=letter.lower()
if new_word:
tokenized_string.append(new_word)
return tokenized_string
def tokenize(string: str) -> list:
tokenized_string = []
for word in string.split():
new_word = ""
for letter in word:
if letter.isalpha():
new_word +=letter.lower()
if new_word:
tokenized_string.append(new_word)
return tokenized_string.sort()
print(tokenize(text))
def tokenize(string: str) -> list:
tokenized_string = []
for word in string.split():
new_word = ""
for letter in word:
if letter.isalpha():
new_word +=letter.lower()
if new_word:
tokenized_string.append(new_word)
tokenized_string.sort()
return tokenized_string
print(tokenize(text))
print(tokenize(text))
tokenize(text)
word_frequencies = {word: tokens.count(word) for word in tokens}
print(word_frequencies)
word_frequencies2 = {word: tokens.count(word) for word in tokens if k >1}
print(word_frequencies2)
word_frequencies2 = {word: tokens.count(word) for word in tokens if k >1}
word_frequencies2 = {word: tokens.count(word) for word in tokens if word >1}
print(word_frequencies2)
word_frequencies2 = {word: tokens.count(word) for word in tokens if tokens.count(word) >1}
print(word_frequencies2)
def token_counts(string: str, k: int = 1) -> dict:
word_frequencies = {word: string.count(word) for word in string if string.count(word) > k}
return word_frequencies
token_counts(tokens)
token_counts(tokens)
text_hist = {'the': 2, 'quick': 1, 'brown': 1, 'fox': 1, 'jumps': 1, 'over': 1, 'lazy': 1, 'dog': 1}
all(text_hist[key] == value for key, value in token_counts(text).items())
token_to_id = {word: tokens[word] for word in tokens.sort()}
token_to_id = {word: index for index, word in tokens.sort()}
token_to_id = {word: index for index, word in tokens}
token_to_id = {word: index for index, word in tokens.sort()}
token_to_id = {word: index for index, word in sorted(tokens)}
token_to_id = {word: tokens[word] for word in sorted(tokens)}
token_to_id = {word: tokens[word] for word in sorted(tokens)}
token_to_id = {word: index for index, word in enumerate(sorted(tokens))}
print(token_to_id)
token_to_id = {word: index for index, word in enumerate(tokens)}
print(token_to_id)
id_to_token = {index: word for index, word in enumerate(tokens)}
assert id_to_token[token_to_id['dog']] == 'dog'
assert token_to_id[id_to_token[4]] == 4
assert all(id_to_token[token_to_id[key]]==key for key in token_to_id) and all(token_to_id[id_to_token[k]]==k for k in range(len(token_to_id)))
assert all(id_to_token[token_to_id[key]]==key for key in token_to_id) and all(token_to_id[id_to_token[k]]==k for k in range(len(token_to_id)))
View(id_to_token)
View(token_to_id)
text = "The quick brown fox jumps over the lazy dog!"
# Write a list comprehension to tokenize the text and remove punctuation
tokens = [''.join(letter.lower() for letter in word if letter.isalpha()) for word in text.split() if any(letter.isalpha() for letter in word)]
# Expected output: ['The', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog']
print(tokens)
token_to_id = {word: index for index, word in enumerate(tokens)}
print(token_to_id)
token_to_id = {word: index for index, word in enumerate(unique(tokens))}
token_to_id = {word: index for index, word in enumerate(np.unique(tokens))}
import numpy as np
token_to_id = {word: index for index, word in enumerate(np.unique(tokens))}
print(token_to_id)
token_to_id = {str(word): index for index, word in enumerate(np.unique(tokens))}
print(token_to_id)
id_to_token = {index: str(word) for index, word in enumerate(np.unique(tokens))}
assert id_to_token[token_to_id['dog']] == 'dog'
assert token_to_id[id_to_token[4]] == 4
# test 3
assert all(id_to_token[token_to_id[key]]==key for key in token_to_id) and all(token_to_id[id_to_token[k]]==k for k in range(len(token_to_id)))
def make_vocabulary_map(documents: list) -> tuple:
# Hint: use your tokenize function
all_tokens = []
for document in documents:
some_tokens = tokenize(documents)
all_tokens.append(some_tokens)
token2id = {str(word): index for index, word in enumerate(np.unique(all_tokens))}
id2token = {index: str(word) for index, word in enumerate(np.unique(all_tokens))}
return token2id, id2token
t2i, i2t = make_vocabulary_map([text])
def make_vocabulary_map(documents: list) -> tuple:
# Hint: use your tokenize function
all_tokens = []
for document in documents:
some_tokens = tokenize(document)
all_tokens.append(some_tokens)
token2id = {str(word): index for index, word in enumerate(np.unique(all_tokens))}
id2token = {index: str(word) for index, word in enumerate(np.unique(all_tokens))}
return token2id, id2token
t2i, i2t = make_vocabulary_map([text])
all(i2t[t2i[tok]] == tok for tok in t2i) # should be True
def tokenize_and_encode(documents: list) -> list:
# Hint: use your make_vocabulary_map and tokenize function
vocab = make_vocabulary_map(documents)[0]
end = []
for document in documents:
tokens = tokenize(document)
ids = []
for token in tokens:
ids.append(vocab[token])
end.append(ids)
return end
enc, t2i, i2t = tokenize_and_encode([text, 'What a luck we had today!'])
all(i2t[t2i[tok]] == tok for tok in t2i) # should be True
def tokenize_and_encode(documents: list) -> list:
# Hint: use your make_vocabulary_map and tokenize function
t2i, i2t = make_vocabulary_map(documents)
end = []
for document in documents:
tokens = tokenize(document)
ids = []
for token in tokens:
ids.append(t2i[token])
end.append(ids)
return end
enc, t2i, i2t = tokenize_and_encode([text, 'What a luck we had today!'])
def tokenize_and_encode(documents: list) -> list:
# Hint: use your make_vocabulary_map and tokenize function
t2i, i2t = make_vocabulary_map(documents)
end = []
for document in documents:
tokens = tokenize(document)
ids = []
for token in tokens:
ids.append(t2i[token])
end.append(ids)
return end, t2i, i2t
enc, t2i, i2t = tokenize_and_encode([text, 'What a luck we had today!'])
def make_vocabulary_map(documents: list) -> tuple:
# Hint: use your tokenize function
all_tokens = []
for document in documents:
some_tokens = tokenize(document)
all_tokens.extend(some_tokens)
token2id = {str(word): index for index, word in enumerate(np.unique(all_tokens))}
id2token = {index: str(word) for index, word in enumerate(np.unique(all_tokens))}
return token2id, id2token
# Test
t2i, i2t = make_vocabulary_map([text])
all(i2t[t2i[tok]] == tok for tok in t2i) # should be True
def tokenize_and_encode(documents: list) -> list:
# Hint: use your make_vocabulary_map and tokenize function
t2i, i2t = make_vocabulary_map(documents)
end = []
for document in documents:
tokens = tokenize(document)
ids = []
for token in tokens:
ids.append(t2i[token])
end.append(ids)
return end, t2i, i2t
# Test:
enc, t2i, i2t = tokenize_and_encode([text, 'What a luck we had today!'])
" | ".join([" ".join(i2t[i] for i in e) for e in enc]) == 'the quick brown fox jumps over the lazy dog | what a luck we had today'
def tokenize_and_encode(documents: list) -> list:
# Hint: use your make_vocabulary_map and tokenize function
t2i, i2t = make_vocabulary_map(documents)
end = []
for document in documents:
tokens = tokenize(document)
ids = []
for token in tokens:
ids.append(t2i[token])
end.append(ids)
return end, t2i, i2t
enc, t2i, i2t = tokenize_and_encode([text, 'What a luck we had today!'])
" | ".join([" ".join(i2t[i] for i in e) for e in enc]) == 'the quick brown fox jumps over the lazy dog | what a luck we had today'
reticulate::repl_python()
